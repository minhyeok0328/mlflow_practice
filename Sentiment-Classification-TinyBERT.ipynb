{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가상환경\n",
    "- python3.10 -m venv .venv\n",
    "- source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 데이터로드 \n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inseop/Desktop/fastapi-backend-bertmodel/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:00<00:00, 37948.12 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:00<00:00, 38430.92 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'negative':0, 'positive':1}\n",
    "\n",
    "dataset = dataset.map(lambda x: {'label':label2id[x['sentiment']]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': [\"I would consider myself a fan of Dean Koontz; having read a number of his novels and liked them all, but unfortunately I never got around to reading Watchers so I'm left with no choice but to rate this film on it's own merits rather than comparing it to the book that I haven't read. I went into this expecting something awful, and while I didn't exactly get a brilliant horror film; I am lead to believe that it's fans of the book that are rating it down because as a film in it's own right, Watchers is an entertaining and somewhat original little horror movie. The plot obviously takes some influence from Predator and begins with an explosion at a research lab. It's not long before a rancher is killed by some strange beast and the boyfriend of the dead man's daughter has picked up an ultra-intelligent runaway dog. A secret Government agency is soon on the case, as the murders continue. The boy continues to be fascinated by the dog's intelligence, but it somehow ties in with the murders and the agency is soon on his tail too.<br /><br />The script for this film was originally written by Paul Haggis, who later disowned it. I don't know why \\x96 the writing here is nowhere near as ridiculous as his 2004 hit Crash! Anyway, the main reason this film works is undoubtedly the dog, who aside from being rather cute, is also the best actor in the film. Corey Haim, hot off the success of The Lost Boys is the human lead and actually has a rather good chemistry with the dog, although it is a little bit ridiculous seeing him talk to it most of the way through the film. The plot is rather convoluted and as such the film is more than a little bit messy; but the ridiculousness of it all pulls it through during the more awkward moments. Michael Ironside also appears in the film and does well as the 'bad cop' side of the Government agents. The monster is, of course, one of the most interesting things about the film and the way it goes around killing people is always entertaining and gory; although unfortunately we don't get to see a lot of it and when we finally do it's rather disappointing - obviously the filmmakers had seen Bigfoot and the Hendersons! Still, this is the sort of film that can be easily enjoyed despite the numerous problems and I'd recommend to any undiscerning viewer of eighties horror.\",\n",
       "  'With the exception of the main character, the acting didn\\'t convince me, but the story was quite good: It\\'s about a love affair between a gay party boy and a young Mormon missionary. As you can imagine, such a relationship is quite problematic. The movie is very American and, as such, has some metaphysical undertones to counterbalance its criticism of religious intolerance. But some story lines are hilarious: one of the main characters asks his gay colleague: \"Do you believe in God?\" And he answers: \"\\x85you mean: other than Madonna?\" All in all, the film is not one of my all-time favorites, but the script is really good and I really liked it: it\\'s entertaining and stands up against (religious) prejudice and intolerance. I think this is an important message in our times. Since I rated films that I liked less with 9, this is clearly a 10 for me. Some people may find this exaggerated, but I think this film deserves it.',\n",
       "  'I found this gem in a rack the local video rental store had of tapes which are exchanged among various rental outlets. \\'The Man who Skied Down Everest\\'. Hmm... never heard about it. The box reads of some Japanese fellow who always wanted to ski down Everest and actually did it. Sounds interesting. I rented it. As expected it was documentary style. The first part can be summarized so: \"I always wanted to ski down mount Everest\". This is followed by some footage of preparation for the event. LOTS of preparation footage. OK, I suppose it takes a lot of preparation. Then we are treated to a protracted piece on the skier, Yuichiro Miura\\'s philosophy on life etc. More filler follows and I begin to wonder where the skiing fits in to this show. More preparation is shown and they begin to make the trip to the mountain. More philosophy is shown. At last they arrive at the mountain and maybe perhaps he will get around to skiing down the friggin\\' thing. Lots of climbing footage later there is a description of the parachute device intended to slow Miuras\\' speed on the steep slope. Finally he straps on the skis and gets ready to go.<br /><br />He\\'s off... He skis about twenty feet and his skis shoot out from under him, he deploys the parachute and tumbles in an inglorious bundle for some distance down the mountain and that\\'s that. End of story. What the heck was that?<br /><br />OK I can buy that he always wanted to ski down Everest, made extensive preparations and actually tried it with camera crew in tow. It didn\\'t work and he ended up tumbling down and almost killing himself, so what egregious hubris would inspire the man to release a film of it and call it skiing down Everest? Perhaps the title,\"The Man Who\\'s Feet Shot Out From Under Him and He Slid On His Ass Down Everest\" was just too long for the tape box.',\n",
       "  'I recently bought this movie on DVD at a discount store for $5. Although it is a no-frills DVD on the Geneon label (just the movie that starts playing immediately - no menu, no special features) the picture and sound quality were EXCELLENT. The movie is based on the true story of one of the biggest bank robberies in history.<br /><br />Richard Jordan, who I must admit to not having heard of, plays the lead - Pinky Green. A charming young man who had spent too much of his few years in prison and now wanted to go straight but is not allowed to do so! He portrays an American in England. David Niven plays the lead bad guy, also with the great charm for which he is famous. Bad, but with scruples as when he refuses to deny Pinky his \"whack\" for the job. Whack, in England, apparently is the fair share of the take and not a bullet in the head as in American gangster films! All the supporting cast do an excellent job producing a very believable movie.<br /><br />What is perhaps best, to me, is that the whole movie is quite enjoyable and understandable (I frequently find myself lost in plot confusions and various characters) without ANY special effects. NO blood. No violence. Not even a single car chase! Just a well written story, well acted, well directed and well photographed! If I had any complaints about the movie, I would question the music. WHAT is bluegrass music doing in a bank heist story that takes place in England?',\n",
       "  '....as to the level of wit on which this comedy operates. Barely even reaching feature length, \"Can I Do It....\\'Till I Need Glasses\" is a collection of (mostly) dirty jokes. Many of them are so short that you can\\'t believe it when you realize that THAT was supposed to be the punchline (example: the Santa Claus gag); others are so long that you can\\'t believe it when you realize that they needed so much time to set up THAT punchline (example: the students\\' awards gag). And nearly all are directed without any artistry. Don\\'t get me wrong: about 1 every 10 jokes actually manages to be funny (the iron / phone one is probably my favorite). There is also some wonderful full-frontal nudity that proves, yet again, that the female body, especially in its natural form, is the best thing on this planet (there is some comedic male nudity as well). And I agree with others that the intentionally stupid title song is actually pretty damn catchy! But none of those reasons are enough to give this film anything more than * out of 4.'],\n",
       " 'sentiment': ['positive', 'positive', 'negative', 'positive', 'negative'],\n",
       " 'label': [1, 1, 0, 1, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inseop/Desktop/fastapi-backend-bertmodel/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hugging Face에 저장되어 있는 모델을 불러올게요. TinyBERT 모델을 불러오겠습니다.\n",
    "model = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I would consider myself a fan of Dean Koontz; having read a number of his novels and liked them all, but unfortunately I never got around to reading Watchers so I'm left with no choice but to rate this film on it's own merits rather than comparing it to the book that I haven't read. I went into this expecting something awful, and while I didn't exactly get a brilliant horror film; I am lead to believe that it's fans of the book that are rating it down because as a film in it's own right, Watchers is an entertaining and somewhat original little horror movie. The plot obviously takes some influence from Predator and begins with an explosion at a research lab. It's not long before a rancher is killed by some strange beast and the boyfriend of the dead man's daughter has picked up an ultra-intelligent runaway dog. A secret Government agency is soon on the case, as the murders continue. The boy continues to be fascinated by the dog's intelligence, but it somehow ties in with the murders and the agency is soon on his tail too.<br /><br />The script for this film was originally written by Paul Haggis, who later disowned it. I don't know why \\x96 the writing here is nowhere near as ridiculous as his 2004 hit Crash! Anyway, the main reason this film works is undoubtedly the dog, who aside from being rather cute, is also the best actor in the film. Corey Haim, hot off the success of The Lost Boys is the human lead and actually has a rather good chemistry with the dog, although it is a little bit ridiculous seeing him talk to it most of the way through the film. The plot is rather convoluted and as such the film is more than a little bit messy; but the ridiculousness of it all pulls it through during the more awkward moments. Michael Ironside also appears in the film and does well as the 'bad cop' side of the Government agents. The monster is, of course, one of the most interesting things about the film and the way it goes around killing people is always entertaining and gory; although unfortunately we don't get to see a lot of it and when we finally do it's rather disappointing - obviously the filmmakers had seen Bigfoot and the Hendersons! Still, this is the sort of film that can be easily enjoyed despite the numerous problems and I'd recommend to any undiscerning viewer of eighties horror.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2052, 5136, 2870, 1037, 5470, 1997, 4670, 12849, 12162, 2480, 1025, 2383, 3191, 1037, 2193, 1997, 2010, 6002, 1998, 4669, 2068, 2035, 1010, 2021, 6854, 1045, 2196, 2288, 2105, 2000, 3752, 3422, 2545, 2061, 1045, 1005, 1049, 2187, 2007, 2053, 3601, 2021, 2000, 3446, 2023, 2143, 2006, 2009, 1005, 1055, 2219, 22617, 2738, 2084, 13599, 2009, 2000, 1996, 2338, 2008, 1045, 4033, 1005, 1056, 3191, 1012, 1045, 2253, 2046, 2023, 8074, 2242, 9643, 1010, 1998, 2096, 1045, 2134, 1005, 1056, 3599, 2131, 1037, 8235, 5469, 2143, 1025, 1045, 2572, 2599, 2000, 2903, 2008, 2009, 1005, 1055, 4599, 1997, 1996, 2338, 2008, 2024, 5790, 2009, 2091, 2138, 2004, 1037, 2143, 1999, 2009, 1005, 1055, 2219, 2157, 1010, 3422, 2545, 2003, 2019, 14036, 1998, 5399, 2434, 2210, 5469, 3185, 1012, 1996, 5436, 5525, 3138, 2070, 3747, 2013, 15267, 1998, 4269, 2007, 2019, 7738, 2012, 1037, 2470, 6845, 1012, 2009, 1005, 1055, 2025, 2146, 2077, 1037, 8086, 2121, 2003, 2730, 2011, 2070, 4326, 6841, 1998, 1996, 6898, 1997, 1996, 2757, 2158, 1005, 1055, 2684, 2038, 3856, 2039, 2019, 11087, 1011, 9414, 19050, 3899, 1012, 1037, 3595, 2231, 4034, 2003, 2574, 2006, 1996, 2553, 1010, 2004, 1996, 9916, 3613, 1012, 1996, 2879, 4247, 2000, 2022, 15677, 2011, 1996, 3899, 1005, 1055, 4454, 1010, 2021, 2009, 5064, 7208, 1999, 2007, 1996, 9916, 1998, 1996, 4034, 2003, 2574, 2006, 2010, 5725, 2205, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5896, 2005, 2023, 2143, 2001, 2761, 2517, 2011, 2703, 5292, 13871, 2483, 1010, 2040, 2101, 4487, 6499, 7962, 2098, 2009, 1012, 1045, 2123, 1005, 1056, 2113, 2339, 1996, 3015, 2182, 2003, 7880, 2379, 2004, 9951, 2004, 2010, 2432, 2718, 5823, 999, 4312, 1010, 1996, 2364, 3114, 2023, 2143, 2573, 2003, 17319, 1996, 3899, 1010, 2040, 4998, 2013, 2108, 2738, 10140, 1010, 2003, 2036, 1996, 2190, 3364, 1999, 1996, 2143, 1012, 18132, 15030, 2213, 1010, 2980, 2125, 1996, 3112, 1997, 1996, 2439, 3337, 2003, 1996, 2529, 2599, 1998, 2941, 2038, 1037, 2738, 2204, 6370, 2007, 1996, 3899, 1010, 2348, 2009, 2003, 1037, 2210, 2978, 9951, 3773, 2032, 2831, 2000, 2009, 2087, 1997, 1996, 2126, 2083, 1996, 2143, 1012, 1996, 5436, 2003, 2738, 9530, 6767, 7630, 3064, 1998, 2004, 2107, 1996, 2143, 2003, 2062, 2084, 1037, 2210, 2978, 18307, 1025, 2021, 1996, 9951, 2791, 1997, 2009, 2035, 8005, 2009, 2083, 2076, 1996, 2062, 9596, 5312, 1012, 2745, 3707, 7363, 2036, 3544, 1999, 1996, 2143, 1998, 2515, 2092, 2004, 1996, 1005, 2919, 8872, 1005, 2217, 1997, 1996, 2231, 6074, 1012, 1996, 6071, 2003, 1010, 1997, 2607, 1010, 2028, 1997, 1996, 2087, 5875, 2477, 2055, 1996, 2143, 1998, 1996, 2126, 2009, 3632, 2105, 4288, 2111, 2003, 2467, 14036, 1998, 2175, 2854, 1025, 2348, 6854, 2057, 2123, 1005, 1056, 2131, 2000, 2156, 1037, 2843, 1997, 2009, 1998, 2043, 2057, 2633, 2079, 2009, 1005, 1055, 2738, 15640, 1011, 5525, 1996, 16587, 2018, 2464, 2502, 13064, 1998, 1996, 9481, 2015, 999, 2145, 1010, 2023, 2003, 1996, 4066, 1997, 2143, 2008, 2064, 2022, 4089, 5632, 2750, 1996, 3365, 3471, 1998, 1045, 1005, 1040, 16755, 2000, 2151, 6151, 2483, 17119, 5582, 13972, 1997, 27690, 5469, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train'][0]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2651, 2003, 6928, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Today is monday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:08<00:00, 4305.10 examples/s]\n",
      "Map: 100%|██████████| 15000/15000 [00:03<00:00, 4158.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    temp = tokenizer(batch['review'], padding=True, truncation=True, max_length=300)\n",
    "    return temp\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"I would consider myself a fan of Dean Koontz; having read a number of his novels and liked them all, but unfortunately I never got around to reading Watchers so I'm left with no choice but to rate this film on it's own merits rather than comparing it to the book that I haven't read. I went into this expecting something awful, and while I didn't exactly get a brilliant horror film; I am lead to believe that it's fans of the book that are rating it down because as a film in it's own right, Watchers is an entertaining and somewhat original little horror movie. The plot obviously takes some influence from Predator and begins with an explosion at a research lab. It's not long before a rancher is killed by some strange beast and the boyfriend of the dead man's daughter has picked up an ultra-intelligent runaway dog. A secret Government agency is soon on the case, as the murders continue. The boy continues to be fascinated by the dog's intelligence, but it somehow ties in with the murders and the agency is soon on his tail too.<br /><br />The script for this film was originally written by Paul Haggis, who later disowned it. I don't know why \\x96 the writing here is nowhere near as ridiculous as his 2004 hit Crash! Anyway, the main reason this film works is undoubtedly the dog, who aside from being rather cute, is also the best actor in the film. Corey Haim, hot off the success of The Lost Boys is the human lead and actually has a rather good chemistry with the dog, although it is a little bit ridiculous seeing him talk to it most of the way through the film. The plot is rather convoluted and as such the film is more than a little bit messy; but the ridiculousness of it all pulls it through during the more awkward moments. Michael Ironside also appears in the film and does well as the 'bad cop' side of the Government agents. The monster is, of course, one of the most interesting things about the film and the way it goes around killing people is always entertaining and gory; although unfortunately we don't get to see a lot of it and when we finally do it's rather disappointing - obviously the filmmakers had seen Bigfoot and the Hendersons! Still, this is the sort of film that can be easily enjoyed despite the numerous problems and I'd recommend to any undiscerning viewer of eighties horror.\",\n",
       " 'sentiment': 'positive',\n",
       " 'label': 1,\n",
       " 'input_ids': [101,\n",
       "  1045,\n",
       "  2052,\n",
       "  5136,\n",
       "  2870,\n",
       "  1037,\n",
       "  5470,\n",
       "  1997,\n",
       "  4670,\n",
       "  12849,\n",
       "  12162,\n",
       "  2480,\n",
       "  1025,\n",
       "  2383,\n",
       "  3191,\n",
       "  1037,\n",
       "  2193,\n",
       "  1997,\n",
       "  2010,\n",
       "  6002,\n",
       "  1998,\n",
       "  4669,\n",
       "  2068,\n",
       "  2035,\n",
       "  1010,\n",
       "  2021,\n",
       "  6854,\n",
       "  1045,\n",
       "  2196,\n",
       "  2288,\n",
       "  2105,\n",
       "  2000,\n",
       "  3752,\n",
       "  3422,\n",
       "  2545,\n",
       "  2061,\n",
       "  1045,\n",
       "  1005,\n",
       "  1049,\n",
       "  2187,\n",
       "  2007,\n",
       "  2053,\n",
       "  3601,\n",
       "  2021,\n",
       "  2000,\n",
       "  3446,\n",
       "  2023,\n",
       "  2143,\n",
       "  2006,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2219,\n",
       "  22617,\n",
       "  2738,\n",
       "  2084,\n",
       "  13599,\n",
       "  2009,\n",
       "  2000,\n",
       "  1996,\n",
       "  2338,\n",
       "  2008,\n",
       "  1045,\n",
       "  4033,\n",
       "  1005,\n",
       "  1056,\n",
       "  3191,\n",
       "  1012,\n",
       "  1045,\n",
       "  2253,\n",
       "  2046,\n",
       "  2023,\n",
       "  8074,\n",
       "  2242,\n",
       "  9643,\n",
       "  1010,\n",
       "  1998,\n",
       "  2096,\n",
       "  1045,\n",
       "  2134,\n",
       "  1005,\n",
       "  1056,\n",
       "  3599,\n",
       "  2131,\n",
       "  1037,\n",
       "  8235,\n",
       "  5469,\n",
       "  2143,\n",
       "  1025,\n",
       "  1045,\n",
       "  2572,\n",
       "  2599,\n",
       "  2000,\n",
       "  2903,\n",
       "  2008,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  4599,\n",
       "  1997,\n",
       "  1996,\n",
       "  2338,\n",
       "  2008,\n",
       "  2024,\n",
       "  5790,\n",
       "  2009,\n",
       "  2091,\n",
       "  2138,\n",
       "  2004,\n",
       "  1037,\n",
       "  2143,\n",
       "  1999,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2219,\n",
       "  2157,\n",
       "  1010,\n",
       "  3422,\n",
       "  2545,\n",
       "  2003,\n",
       "  2019,\n",
       "  14036,\n",
       "  1998,\n",
       "  5399,\n",
       "  2434,\n",
       "  2210,\n",
       "  5469,\n",
       "  3185,\n",
       "  1012,\n",
       "  1996,\n",
       "  5436,\n",
       "  5525,\n",
       "  3138,\n",
       "  2070,\n",
       "  3747,\n",
       "  2013,\n",
       "  15267,\n",
       "  1998,\n",
       "  4269,\n",
       "  2007,\n",
       "  2019,\n",
       "  7738,\n",
       "  2012,\n",
       "  1037,\n",
       "  2470,\n",
       "  6845,\n",
       "  1012,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  2146,\n",
       "  2077,\n",
       "  1037,\n",
       "  8086,\n",
       "  2121,\n",
       "  2003,\n",
       "  2730,\n",
       "  2011,\n",
       "  2070,\n",
       "  4326,\n",
       "  6841,\n",
       "  1998,\n",
       "  1996,\n",
       "  6898,\n",
       "  1997,\n",
       "  1996,\n",
       "  2757,\n",
       "  2158,\n",
       "  1005,\n",
       "  1055,\n",
       "  2684,\n",
       "  2038,\n",
       "  3856,\n",
       "  2039,\n",
       "  2019,\n",
       "  11087,\n",
       "  1011,\n",
       "  9414,\n",
       "  19050,\n",
       "  3899,\n",
       "  1012,\n",
       "  1037,\n",
       "  3595,\n",
       "  2231,\n",
       "  4034,\n",
       "  2003,\n",
       "  2574,\n",
       "  2006,\n",
       "  1996,\n",
       "  2553,\n",
       "  1010,\n",
       "  2004,\n",
       "  1996,\n",
       "  9916,\n",
       "  3613,\n",
       "  1012,\n",
       "  1996,\n",
       "  2879,\n",
       "  4247,\n",
       "  2000,\n",
       "  2022,\n",
       "  15677,\n",
       "  2011,\n",
       "  1996,\n",
       "  3899,\n",
       "  1005,\n",
       "  1055,\n",
       "  4454,\n",
       "  1010,\n",
       "  2021,\n",
       "  2009,\n",
       "  5064,\n",
       "  7208,\n",
       "  1999,\n",
       "  2007,\n",
       "  1996,\n",
       "  9916,\n",
       "  1998,\n",
       "  1996,\n",
       "  4034,\n",
       "  2003,\n",
       "  2574,\n",
       "  2006,\n",
       "  2010,\n",
       "  5725,\n",
       "  2205,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1996,\n",
       "  5896,\n",
       "  2005,\n",
       "  2023,\n",
       "  2143,\n",
       "  2001,\n",
       "  2761,\n",
       "  2517,\n",
       "  2011,\n",
       "  2703,\n",
       "  5292,\n",
       "  13871,\n",
       "  2483,\n",
       "  1010,\n",
       "  2040,\n",
       "  2101,\n",
       "  4487,\n",
       "  6499,\n",
       "  7962,\n",
       "  2098,\n",
       "  2009,\n",
       "  1012,\n",
       "  1045,\n",
       "  2123,\n",
       "  1005,\n",
       "  1056,\n",
       "  2113,\n",
       "  2339,\n",
       "  1996,\n",
       "  3015,\n",
       "  2182,\n",
       "  2003,\n",
       "  7880,\n",
       "  2379,\n",
       "  2004,\n",
       "  9951,\n",
       "  2004,\n",
       "  2010,\n",
       "  2432,\n",
       "  2718,\n",
       "  5823,\n",
       "  999,\n",
       "  4312,\n",
       "  1010,\n",
       "  1996,\n",
       "  2364,\n",
       "  3114,\n",
       "  2023,\n",
       "  2143,\n",
       "  2573,\n",
       "  2003,\n",
       "  17319,\n",
       "  1996,\n",
       "  3899,\n",
       "  1010,\n",
       "  2040,\n",
       "  4998,\n",
       "  2013,\n",
       "  2108,\n",
       "  2738,\n",
       "  10140,\n",
       "  1010,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1) # 예측값과 실제 레이블 데이터를 튜플로 입력\n",
    "\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0:'negative', 1:'positive'}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model,\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inseop/Desktop/fastapi-backend-bertmodel/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# 모델 학습을 위한 하이퍼파라미터와 설정 정의\n",
    "args = TrainingArguments(\n",
    "    output_dir='train_dir',               # 학습 결과를 저장할 디렉터리\n",
    "    overwrite_output_dir=True,            # 출력 디렉터리에 이미 있는 파일을 덮어쓸지 여부\n",
    "    num_train_epochs=3,                   # 학습할 에포크(epoch) 수\n",
    "    learning_rate=2e-5,                   # 학습률 (learning rate)\n",
    "    per_device_train_batch_size=32,       # 각 디바이스(예: GPU)당 학습 배치 크기\n",
    "    per_device_eval_batch_size=32,        # 각 디바이스당 평가 배치 크기\n",
    "    evaluation_strategy='epoch'           # 평가 전략 (여기서는 매 에포크마다 평가)\n",
    ")\n",
    "\n",
    "# Trainer 객체를 생성하여 학습 및 평가를 관리\n",
    "trainer = Trainer(\n",
    "    model=model,                          # 학습할 모델\n",
    "    args=args,                            # 학습 파라미터 설정\n",
    "    train_dataset=dataset['train'],       # 학습에 사용할 데이터셋\n",
    "    eval_dataset=dataset['test'],         # 평가에 사용할 데이터셋\n",
    "    compute_metrics=compute_metrics,      # 평가 지표를 계산하는 함수\n",
    "    tokenizer=tokenizer                   # 토크나이저 (텍스트를 토큰으로 변환하는 도구)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 500/3282 [02:33<13:21,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.472, 'grad_norm': 10.08832836151123, 'learning_rate': 1.695307739183425e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1000/3282 [04:59<11:12,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3659, 'grad_norm': 7.427433967590332, 'learning_rate': 1.3906154783668494e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 33%|███▎      | 1094/3282 [06:04<21:44,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3036186397075653, 'eval_accuracy': 0.8712, 'eval_runtime': 36.317, 'eval_samples_per_second': 413.03, 'eval_steps_per_second': 12.914, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 1500/3282 [08:01<08:29,  3.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3148, 'grad_norm': 9.545378684997559, 'learning_rate': 1.0859232175502743e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 2000/3282 [10:25<06:06,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2927, 'grad_norm': 18.911134719848633, 'learning_rate': 7.81230956733699e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 2188/3282 [11:55<04:47,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2777292728424072, 'eval_accuracy': 0.8813333333333333, 'eval_runtime': 35.2961, 'eval_samples_per_second': 424.976, 'eval_steps_per_second': 13.288, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 2500/3282 [13:24<03:44,  3.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2765, 'grad_norm': 18.128141403198242, 'learning_rate': 4.765386959171238e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 3000/3282 [15:48<01:19,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2618, 'grad_norm': 12.661847114562988, 'learning_rate': 1.7184643510054846e-06, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 3282/3282 [17:45<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27656206488609314, 'eval_accuracy': 0.8856, 'eval_runtime': 35.628, 'eval_samples_per_second': 421.017, 'eval_steps_per_second': 13.164, 'epoch': 3.0}\n",
      "{'train_runtime': 1065.8079, 'train_samples_per_second': 98.517, 'train_steps_per_second': 3.079, 'train_loss': 0.3241748635468143, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3282, training_loss=0.3241748635468143, metrics={'train_runtime': 1065.8079, 'train_samples_per_second': 98.517, 'train_steps_per_second': 3.079, 'total_flos': 882184338000000.0, 'train_loss': 0.3241748635468143, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU vs GPU의 차이\n",
    "\n",
    "# 간단한 덧셈 문제 100개 문제를 누가 더 빨리 풀까요?\n",
    "# - 대학생 1명(CPU) vs 초딩 100명(GPU)\n",
    "# - NVIDIA가 왜 미친듯이 올랐죠? => GPU만들 잖아요. \n",
    "\n",
    "# GPU랑 딥러닝(단순 행렬 계산)은 뭔 상관이지?\n",
    "# - 간단한 덧셈 문제 푸는데 대학생 1명 (시급 100만원) -> 초딩100명(시급1만원)\n",
    "\n",
    "# 다굴엔 장사없다. => 포폴이 졸라 많으면 되요. 포폴 1개 인것 보다 2개 => 3개 => 4개 // 앱 10개(계산기...) => 코테 안봅니다.\n",
    "# 회사에서 메일이 올 때 코테(필터) 없이 바로 면접을 보자고 해요.\n",
    "# 코테 30분 미만으로 머리 식히는 용으로 => 나머지 진짜 공부를 하세요.\n",
    "# DE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('tinybert-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "classifier = pipeline('text-classification', model='tinybert-sentiment-analysis', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.6487680673599243},\n",
       " {'label': 'positive', 'score': 0.7922708988189697},\n",
       " {'label': 'positive', 'score': 0.9265885353088379},\n",
       " {'label': 'positive', 'score': 0.6664153337478638}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    \"The Chinese WW2 spy thriller “Decoded” stands out for a number of reasons, mostly in spite of its conventional and hackneyed depiction of a troubled mathematician who deciphers encrypted messages for the mainland army. For starters, “Decoded” provides a dramatic change of pace for two marquee-worthy names: soft-spoken heart-throb Liu Haoran, who takes an unusual leading man role as the gifted, but painfully shy codebreaker Rong Jinzhen; and director Chen Sicheng, who’s best known for his goofy mega-blockbuster “Detective Chinatown” comedies. With “Decoded,” a plodding adaptation of Mai Jia’s popular source novel, Chen and Liu abandon cheap-seats humor—Liu co-starred in the “Detective Chinatown” movies, playing a straight man to comedian Wang Baoqiang—to pursue a more sober, but less convincing type of cornball power fantasy.\",\n",
    "    \"Liu also played a frustrated, but superhumanly gifted wallflower in “Detective Chinatown.” He was more convincing in those movies, partly because he was part of a winning buddy duo, but also because he wasn’t trying to capital-A act while wearing hairpieces, whose synthetic hairs thin at an alarming rate as his character ages. As Jinzhen, Liu brings to mind Russell Crowe’s performance as the schizophrenic mathematician John Nash in “A Beautiful Mind.” That association gets harder and harder to shake as Jinzhen inevitably loses his grip on reality while trying to solve the Black Cipher, a nigh-impossible encryption key that was specifically designed to stump Jinzhen.\",\n",
    "    \"Liu’s mostly compelling as a leading man whenever he can suggest a lot about Jinzhen by speaking softly and deferring his gaze, as if Jinzhen expects to be reprimanded or inconvenienced at any time. He’s still often eclipsed by co-star John Cusack, whose broad and twitchy performance often distracts from his dialogue, as well as a series of campy dream sequences that ostensibly speak for Liu’s introverted protagonist.\",\n",
    "    \"Liu doesn’t exactly light up the screen in his limited capacity as a humanoid plot device. His character either reacts to or follows after whatever promising new development might help Jinzhen to solve the latest problem that’s vexing him. The filmmakers do what they can to compensate for their unlikely hero’s prevailing lack of charm and agency, but not even the combined forces of Lloyd Dobler and the Fab Four can bring a spike of joy to this DOA period drama.\"\n",
    "]\n",
    "\n",
    "classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9783684015274048}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[\n",
    "    \"\"\"When are you guys going to fix all the issues?? Firstly, none of the reaction emojis are showing up. It's just a grey circle. When scrolling, it doesn't move freely. There's like a delay!! Very frustrating!!! Also, nearly every post is either from a \"suggested page\" or a \"sponsered page\". I hardly ever see anything from the pages that I actually follow or my friends pages. No wonder so many people are leaving FB 🙄🙄\"\"\"\n",
    "]\n",
    "\n",
    "classifier(data)\n",
    "\n",
    "# 프로그램: 구글 플레이 스토어 링크를 넣으면 => 리뷰 데이터 전체 크롤링 => 부정의 강도가 0.8 이상인 의견만 필터 걸어서 고객사에게 공유\n",
    "# 별1개에 네거티브 0.8이상 => slack 으로 알림보내 => 대응"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 s3에 업로드\n",
    "# (1) AWS 로그인 한 다음 => 버킷 생성\n",
    "# (2) boto3를 활용해서 코드 베이스 s3 생성 및 파일 업로드\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS\n",
    "- IAM 계정 / EC2 - pem키 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3') # s3 만들 수 있는 권한 / 계정(aws credentials)\n",
    "\n",
    "# s3.list_buckets()\n",
    "# s3.create_bucket()\n",
    "\n",
    "# 어떤 원리로 실행이 되는 걸까요?\n",
    "\n",
    "# aws credentials\n",
    "# aws configure list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 bucket 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3 = boto3.client('s3') # s3 콘솔에 접속\n",
    "bucket_name = 'inseop-mlmodels' # bucket name\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    response = s3.list_buckets()\n",
    "\n",
    "    bucket_list = []\n",
    "    for buck in response['Buckets']:\n",
    "        bucket_list.append(buck['Name'])\n",
    "\n",
    "    if bucket_name not in bucket_list:\n",
    "        try:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint':'ap-northeast-2'}\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            print('오류 발생 :', e)\n",
    "\n",
    "            if e.response['Error']['Code'] == 'BucketAlreadyExists':    \n",
    "                print('다른 버킷 이름을 입력하세요.')\n",
    "\n",
    "            elif e.reponse['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "                print('이미 만들어 져있는 버킷입니다.')\n",
    "                \n",
    "            else:\n",
    "                print('버킷 만들기를 재시도 중입니다.')\n",
    "                time.sleep(3)\n",
    "                create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bucket('inseop-mlmodelss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE => 데이터가 유실되었을 때 어떻게 복구할 것인가\n",
    "# AB180 => 맨날 복구. 더미 데이터에서 데이터 불러와서 => 다시 재가공 => 다시 DB insert\n",
    "# 하루, 한시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3에 파일(폴더) 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'inseop-mlmodels'\n",
    "\n",
    "# s3.upload_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for root, dir, files in os.walk('tinybert-sentiment-analysis'):\n",
    "    # print(root, dir, files)\n",
    "\n",
    "    for file_name in files:\n",
    "        # print(file_name)\n",
    "\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        print(file_path)\n",
    "    \n",
    "        # s3.upload_file(file_path, bucket_name, file_name)\n",
    "        s3.upload_file(file_path, bucket_name, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def s3_upload_file_folder_name(model_folder, folder_name):\n",
    "    for root, dir, files in os.walk(model_folder):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            s3_key = os.path.join(folder_name, file_name)\n",
    "            s3.upload_file(file_path, bucket_name, s3_key)\n",
    "            # EC2에다가 docker => EC2 - AWS CLI(credentials은 필요X) + Docker pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_upload_file_folder_name('tinybert-sentiment-analysis', 'tinybert-test-folder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
