{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Í∞ÄÏÉÅÌôòÍ≤Ω\n",
    "- python3.10 -m venv .venv\n",
    "- source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Îç∞Ïù¥ÌÑ∞Î°úÎìú \n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/All-CSV-ML-Data-Files-Download/master/IMDB-Dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inseop/Desktop/fastapi-backend-bertmodel/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35000/35000 [00:00<00:00, 37948.12 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [00:00<00:00, 38430.92 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {'negative':0, 'positive':1}\n",
    "\n",
    "dataset = dataset.map(lambda x: {'label':label2id[x['sentiment']]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': [\"I would consider myself a fan of Dean Koontz; having read a number of his novels and liked them all, but unfortunately I never got around to reading Watchers so I'm left with no choice but to rate this film on it's own merits rather than comparing it to the book that I haven't read. I went into this expecting something awful, and while I didn't exactly get a brilliant horror film; I am lead to believe that it's fans of the book that are rating it down because as a film in it's own right, Watchers is an entertaining and somewhat original little horror movie. The plot obviously takes some influence from Predator and begins with an explosion at a research lab. It's not long before a rancher is killed by some strange beast and the boyfriend of the dead man's daughter has picked up an ultra-intelligent runaway dog. A secret Government agency is soon on the case, as the murders continue. The boy continues to be fascinated by the dog's intelligence, but it somehow ties in with the murders and the agency is soon on his tail too.<br /><br />The script for this film was originally written by Paul Haggis, who later disowned it. I don't know why \\x96 the writing here is nowhere near as ridiculous as his 2004 hit Crash! Anyway, the main reason this film works is undoubtedly the dog, who aside from being rather cute, is also the best actor in the film. Corey Haim, hot off the success of The Lost Boys is the human lead and actually has a rather good chemistry with the dog, although it is a little bit ridiculous seeing him talk to it most of the way through the film. The plot is rather convoluted and as such the film is more than a little bit messy; but the ridiculousness of it all pulls it through during the more awkward moments. Michael Ironside also appears in the film and does well as the 'bad cop' side of the Government agents. The monster is, of course, one of the most interesting things about the film and the way it goes around killing people is always entertaining and gory; although unfortunately we don't get to see a lot of it and when we finally do it's rather disappointing - obviously the filmmakers had seen Bigfoot and the Hendersons! Still, this is the sort of film that can be easily enjoyed despite the numerous problems and I'd recommend to any undiscerning viewer of eighties horror.\",\n",
       "  'With the exception of the main character, the acting didn\\'t convince me, but the story was quite good: It\\'s about a love affair between a gay party boy and a young Mormon missionary. As you can imagine, such a relationship is quite problematic. The movie is very American and, as such, has some metaphysical undertones to counterbalance its criticism of religious intolerance. But some story lines are hilarious: one of the main characters asks his gay colleague: \"Do you believe in God?\" And he answers: \"\\x85you mean: other than Madonna?\" All in all, the film is not one of my all-time favorites, but the script is really good and I really liked it: it\\'s entertaining and stands up against (religious) prejudice and intolerance. I think this is an important message in our times. Since I rated films that I liked less with 9, this is clearly a 10 for me. Some people may find this exaggerated, but I think this film deserves it.',\n",
       "  'I found this gem in a rack the local video rental store had of tapes which are exchanged among various rental outlets. \\'The Man who Skied Down Everest\\'. Hmm... never heard about it. The box reads of some Japanese fellow who always wanted to ski down Everest and actually did it. Sounds interesting. I rented it. As expected it was documentary style. The first part can be summarized so: \"I always wanted to ski down mount Everest\". This is followed by some footage of preparation for the event. LOTS of preparation footage. OK, I suppose it takes a lot of preparation. Then we are treated to a protracted piece on the skier, Yuichiro Miura\\'s philosophy on life etc. More filler follows and I begin to wonder where the skiing fits in to this show. More preparation is shown and they begin to make the trip to the mountain. More philosophy is shown. At last they arrive at the mountain and maybe perhaps he will get around to skiing down the friggin\\' thing. Lots of climbing footage later there is a description of the parachute device intended to slow Miuras\\' speed on the steep slope. Finally he straps on the skis and gets ready to go.<br /><br />He\\'s off... He skis about twenty feet and his skis shoot out from under him, he deploys the parachute and tumbles in an inglorious bundle for some distance down the mountain and that\\'s that. End of story. What the heck was that?<br /><br />OK I can buy that he always wanted to ski down Everest, made extensive preparations and actually tried it with camera crew in tow. It didn\\'t work and he ended up tumbling down and almost killing himself, so what egregious hubris would inspire the man to release a film of it and call it skiing down Everest? Perhaps the title,\"The Man Who\\'s Feet Shot Out From Under Him and He Slid On His Ass Down Everest\" was just too long for the tape box.',\n",
       "  'I recently bought this movie on DVD at a discount store for $5. Although it is a no-frills DVD on the Geneon label (just the movie that starts playing immediately - no menu, no special features) the picture and sound quality were EXCELLENT. The movie is based on the true story of one of the biggest bank robberies in history.<br /><br />Richard Jordan, who I must admit to not having heard of, plays the lead - Pinky Green. A charming young man who had spent too much of his few years in prison and now wanted to go straight but is not allowed to do so! He portrays an American in England. David Niven plays the lead bad guy, also with the great charm for which he is famous. Bad, but with scruples as when he refuses to deny Pinky his \"whack\" for the job. Whack, in England, apparently is the fair share of the take and not a bullet in the head as in American gangster films! All the supporting cast do an excellent job producing a very believable movie.<br /><br />What is perhaps best, to me, is that the whole movie is quite enjoyable and understandable (I frequently find myself lost in plot confusions and various characters) without ANY special effects. NO blood. No violence. Not even a single car chase! Just a well written story, well acted, well directed and well photographed! If I had any complaints about the movie, I would question the music. WHAT is bluegrass music doing in a bank heist story that takes place in England?',\n",
       "  '....as to the level of wit on which this comedy operates. Barely even reaching feature length, \"Can I Do It....\\'Till I Need Glasses\" is a collection of (mostly) dirty jokes. Many of them are so short that you can\\'t believe it when you realize that THAT was supposed to be the punchline (example: the Santa Claus gag); others are so long that you can\\'t believe it when you realize that they needed so much time to set up THAT punchline (example: the students\\' awards gag). And nearly all are directed without any artistry. Don\\'t get me wrong: about 1 every 10 jokes actually manages to be funny (the iron / phone one is probably my favorite). There is also some wonderful full-frontal nudity that proves, yet again, that the female body, especially in its natural form, is the best thing on this planet (there is some comedic male nudity as well). And I agree with others that the intentionally stupid title song is actually pretty damn catchy! But none of those reasons are enough to give this film anything more than * out of 4.'],\n",
       " 'sentiment': ['positive', 'positive', 'negative', 'positive', 'negative'],\n",
       " 'label': [1, 1, 0, 1, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inseop/Desktop/fastapi-backend-bertmodel/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='huawei-noah/TinyBERT_General_4L_312D', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Hugging FaceÏóê Ï†ÄÏû•ÎêòÏñ¥ ÏûàÎäî Î™®Îç∏ÏùÑ Î∂àÎü¨Ïò¨Í≤åÏöî. TinyBERT Î™®Îç∏ÏùÑ Î∂àÎü¨Ïò§Í≤†ÏäµÎãàÎã§.\n",
    "model = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I would consider myself a fan of Dean Koontz; having read a number of his novels and liked them all, but unfortunately I never got around to reading Watchers so I'm left with no choice but to rate this film on it's own merits rather than comparing it to the book that I haven't read. I went into this expecting something awful, and while I didn't exactly get a brilliant horror film; I am lead to believe that it's fans of the book that are rating it down because as a film in it's own right, Watchers is an entertaining and somewhat original little horror movie. The plot obviously takes some influence from Predator and begins with an explosion at a research lab. It's not long before a rancher is killed by some strange beast and the boyfriend of the dead man's daughter has picked up an ultra-intelligent runaway dog. A secret Government agency is soon on the case, as the murders continue. The boy continues to be fascinated by the dog's intelligence, but it somehow ties in with the murders and the agency is soon on his tail too.<br /><br />The script for this film was originally written by Paul Haggis, who later disowned it. I don't know why \\x96 the writing here is nowhere near as ridiculous as his 2004 hit Crash! Anyway, the main reason this film works is undoubtedly the dog, who aside from being rather cute, is also the best actor in the film. Corey Haim, hot off the success of The Lost Boys is the human lead and actually has a rather good chemistry with the dog, although it is a little bit ridiculous seeing him talk to it most of the way through the film. The plot is rather convoluted and as such the film is more than a little bit messy; but the ridiculousness of it all pulls it through during the more awkward moments. Michael Ironside also appears in the film and does well as the 'bad cop' side of the Government agents. The monster is, of course, one of the most interesting things about the film and the way it goes around killing people is always entertaining and gory; although unfortunately we don't get to see a lot of it and when we finally do it's rather disappointing - obviously the filmmakers had seen Bigfoot and the Hendersons! Still, this is the sort of film that can be easily enjoyed despite the numerous problems and I'd recommend to any undiscerning viewer of eighties horror.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2052, 5136, 2870, 1037, 5470, 1997, 4670, 12849, 12162, 2480, 1025, 2383, 3191, 1037, 2193, 1997, 2010, 6002, 1998, 4669, 2068, 2035, 1010, 2021, 6854, 1045, 2196, 2288, 2105, 2000, 3752, 3422, 2545, 2061, 1045, 1005, 1049, 2187, 2007, 2053, 3601, 2021, 2000, 3446, 2023, 2143, 2006, 2009, 1005, 1055, 2219, 22617, 2738, 2084, 13599, 2009, 2000, 1996, 2338, 2008, 1045, 4033, 1005, 1056, 3191, 1012, 1045, 2253, 2046, 2023, 8074, 2242, 9643, 1010, 1998, 2096, 1045, 2134, 1005, 1056, 3599, 2131, 1037, 8235, 5469, 2143, 1025, 1045, 2572, 2599, 2000, 2903, 2008, 2009, 1005, 1055, 4599, 1997, 1996, 2338, 2008, 2024, 5790, 2009, 2091, 2138, 2004, 1037, 2143, 1999, 2009, 1005, 1055, 2219, 2157, 1010, 3422, 2545, 2003, 2019, 14036, 1998, 5399, 2434, 2210, 5469, 3185, 1012, 1996, 5436, 5525, 3138, 2070, 3747, 2013, 15267, 1998, 4269, 2007, 2019, 7738, 2012, 1037, 2470, 6845, 1012, 2009, 1005, 1055, 2025, 2146, 2077, 1037, 8086, 2121, 2003, 2730, 2011, 2070, 4326, 6841, 1998, 1996, 6898, 1997, 1996, 2757, 2158, 1005, 1055, 2684, 2038, 3856, 2039, 2019, 11087, 1011, 9414, 19050, 3899, 1012, 1037, 3595, 2231, 4034, 2003, 2574, 2006, 1996, 2553, 1010, 2004, 1996, 9916, 3613, 1012, 1996, 2879, 4247, 2000, 2022, 15677, 2011, 1996, 3899, 1005, 1055, 4454, 1010, 2021, 2009, 5064, 7208, 1999, 2007, 1996, 9916, 1998, 1996, 4034, 2003, 2574, 2006, 2010, 5725, 2205, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5896, 2005, 2023, 2143, 2001, 2761, 2517, 2011, 2703, 5292, 13871, 2483, 1010, 2040, 2101, 4487, 6499, 7962, 2098, 2009, 1012, 1045, 2123, 1005, 1056, 2113, 2339, 1996, 3015, 2182, 2003, 7880, 2379, 2004, 9951, 2004, 2010, 2432, 2718, 5823, 999, 4312, 1010, 1996, 2364, 3114, 2023, 2143, 2573, 2003, 17319, 1996, 3899, 1010, 2040, 4998, 2013, 2108, 2738, 10140, 1010, 2003, 2036, 1996, 2190, 3364, 1999, 1996, 2143, 1012, 18132, 15030, 2213, 1010, 2980, 2125, 1996, 3112, 1997, 1996, 2439, 3337, 2003, 1996, 2529, 2599, 1998, 2941, 2038, 1037, 2738, 2204, 6370, 2007, 1996, 3899, 1010, 2348, 2009, 2003, 1037, 2210, 2978, 9951, 3773, 2032, 2831, 2000, 2009, 2087, 1997, 1996, 2126, 2083, 1996, 2143, 1012, 1996, 5436, 2003, 2738, 9530, 6767, 7630, 3064, 1998, 2004, 2107, 1996, 2143, 2003, 2062, 2084, 1037, 2210, 2978, 18307, 1025, 2021, 1996, 9951, 2791, 1997, 2009, 2035, 8005, 2009, 2083, 2076, 1996, 2062, 9596, 5312, 1012, 2745, 3707, 7363, 2036, 3544, 1999, 1996, 2143, 1998, 2515, 2092, 2004, 1996, 1005, 2919, 8872, 1005, 2217, 1997, 1996, 2231, 6074, 1012, 1996, 6071, 2003, 1010, 1997, 2607, 1010, 2028, 1997, 1996, 2087, 5875, 2477, 2055, 1996, 2143, 1998, 1996, 2126, 2009, 3632, 2105, 4288, 2111, 2003, 2467, 14036, 1998, 2175, 2854, 1025, 2348, 6854, 2057, 2123, 1005, 1056, 2131, 2000, 2156, 1037, 2843, 1997, 2009, 1998, 2043, 2057, 2633, 2079, 2009, 1005, 1055, 2738, 15640, 1011, 5525, 1996, 16587, 2018, 2464, 2502, 13064, 1998, 1996, 9481, 2015, 999, 2145, 1010, 2023, 2003, 1996, 4066, 1997, 2143, 2008, 2064, 2022, 4089, 5632, 2750, 1996, 3365, 3471, 1998, 1045, 1005, 1040, 16755, 2000, 2151, 6151, 2483, 17119, 5582, 13972, 1997, 27690, 5469, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset['train'][0]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2651, 2003, 6928, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Today is monday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35000/35000 [00:08<00:00, 4305.10 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15000/15000 [00:03<00:00, 4158.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    temp = tokenizer(batch['review'], padding=True, truncation=True, max_length=300)\n",
    "    return temp\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True, batch_size=None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': \"I would consider myself a fan of Dean Koontz; having read a number of his novels and liked them all, but unfortunately I never got around to reading Watchers so I'm left with no choice but to rate this film on it's own merits rather than comparing it to the book that I haven't read. I went into this expecting something awful, and while I didn't exactly get a brilliant horror film; I am lead to believe that it's fans of the book that are rating it down because as a film in it's own right, Watchers is an entertaining and somewhat original little horror movie. The plot obviously takes some influence from Predator and begins with an explosion at a research lab. It's not long before a rancher is killed by some strange beast and the boyfriend of the dead man's daughter has picked up an ultra-intelligent runaway dog. A secret Government agency is soon on the case, as the murders continue. The boy continues to be fascinated by the dog's intelligence, but it somehow ties in with the murders and the agency is soon on his tail too.<br /><br />The script for this film was originally written by Paul Haggis, who later disowned it. I don't know why \\x96 the writing here is nowhere near as ridiculous as his 2004 hit Crash! Anyway, the main reason this film works is undoubtedly the dog, who aside from being rather cute, is also the best actor in the film. Corey Haim, hot off the success of The Lost Boys is the human lead and actually has a rather good chemistry with the dog, although it is a little bit ridiculous seeing him talk to it most of the way through the film. The plot is rather convoluted and as such the film is more than a little bit messy; but the ridiculousness of it all pulls it through during the more awkward moments. Michael Ironside also appears in the film and does well as the 'bad cop' side of the Government agents. The monster is, of course, one of the most interesting things about the film and the way it goes around killing people is always entertaining and gory; although unfortunately we don't get to see a lot of it and when we finally do it's rather disappointing - obviously the filmmakers had seen Bigfoot and the Hendersons! Still, this is the sort of film that can be easily enjoyed despite the numerous problems and I'd recommend to any undiscerning viewer of eighties horror.\",\n",
       " 'sentiment': 'positive',\n",
       " 'label': 1,\n",
       " 'input_ids': [101,\n",
       "  1045,\n",
       "  2052,\n",
       "  5136,\n",
       "  2870,\n",
       "  1037,\n",
       "  5470,\n",
       "  1997,\n",
       "  4670,\n",
       "  12849,\n",
       "  12162,\n",
       "  2480,\n",
       "  1025,\n",
       "  2383,\n",
       "  3191,\n",
       "  1037,\n",
       "  2193,\n",
       "  1997,\n",
       "  2010,\n",
       "  6002,\n",
       "  1998,\n",
       "  4669,\n",
       "  2068,\n",
       "  2035,\n",
       "  1010,\n",
       "  2021,\n",
       "  6854,\n",
       "  1045,\n",
       "  2196,\n",
       "  2288,\n",
       "  2105,\n",
       "  2000,\n",
       "  3752,\n",
       "  3422,\n",
       "  2545,\n",
       "  2061,\n",
       "  1045,\n",
       "  1005,\n",
       "  1049,\n",
       "  2187,\n",
       "  2007,\n",
       "  2053,\n",
       "  3601,\n",
       "  2021,\n",
       "  2000,\n",
       "  3446,\n",
       "  2023,\n",
       "  2143,\n",
       "  2006,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2219,\n",
       "  22617,\n",
       "  2738,\n",
       "  2084,\n",
       "  13599,\n",
       "  2009,\n",
       "  2000,\n",
       "  1996,\n",
       "  2338,\n",
       "  2008,\n",
       "  1045,\n",
       "  4033,\n",
       "  1005,\n",
       "  1056,\n",
       "  3191,\n",
       "  1012,\n",
       "  1045,\n",
       "  2253,\n",
       "  2046,\n",
       "  2023,\n",
       "  8074,\n",
       "  2242,\n",
       "  9643,\n",
       "  1010,\n",
       "  1998,\n",
       "  2096,\n",
       "  1045,\n",
       "  2134,\n",
       "  1005,\n",
       "  1056,\n",
       "  3599,\n",
       "  2131,\n",
       "  1037,\n",
       "  8235,\n",
       "  5469,\n",
       "  2143,\n",
       "  1025,\n",
       "  1045,\n",
       "  2572,\n",
       "  2599,\n",
       "  2000,\n",
       "  2903,\n",
       "  2008,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  4599,\n",
       "  1997,\n",
       "  1996,\n",
       "  2338,\n",
       "  2008,\n",
       "  2024,\n",
       "  5790,\n",
       "  2009,\n",
       "  2091,\n",
       "  2138,\n",
       "  2004,\n",
       "  1037,\n",
       "  2143,\n",
       "  1999,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2219,\n",
       "  2157,\n",
       "  1010,\n",
       "  3422,\n",
       "  2545,\n",
       "  2003,\n",
       "  2019,\n",
       "  14036,\n",
       "  1998,\n",
       "  5399,\n",
       "  2434,\n",
       "  2210,\n",
       "  5469,\n",
       "  3185,\n",
       "  1012,\n",
       "  1996,\n",
       "  5436,\n",
       "  5525,\n",
       "  3138,\n",
       "  2070,\n",
       "  3747,\n",
       "  2013,\n",
       "  15267,\n",
       "  1998,\n",
       "  4269,\n",
       "  2007,\n",
       "  2019,\n",
       "  7738,\n",
       "  2012,\n",
       "  1037,\n",
       "  2470,\n",
       "  6845,\n",
       "  1012,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  2146,\n",
       "  2077,\n",
       "  1037,\n",
       "  8086,\n",
       "  2121,\n",
       "  2003,\n",
       "  2730,\n",
       "  2011,\n",
       "  2070,\n",
       "  4326,\n",
       "  6841,\n",
       "  1998,\n",
       "  1996,\n",
       "  6898,\n",
       "  1997,\n",
       "  1996,\n",
       "  2757,\n",
       "  2158,\n",
       "  1005,\n",
       "  1055,\n",
       "  2684,\n",
       "  2038,\n",
       "  3856,\n",
       "  2039,\n",
       "  2019,\n",
       "  11087,\n",
       "  1011,\n",
       "  9414,\n",
       "  19050,\n",
       "  3899,\n",
       "  1012,\n",
       "  1037,\n",
       "  3595,\n",
       "  2231,\n",
       "  4034,\n",
       "  2003,\n",
       "  2574,\n",
       "  2006,\n",
       "  1996,\n",
       "  2553,\n",
       "  1010,\n",
       "  2004,\n",
       "  1996,\n",
       "  9916,\n",
       "  3613,\n",
       "  1012,\n",
       "  1996,\n",
       "  2879,\n",
       "  4247,\n",
       "  2000,\n",
       "  2022,\n",
       "  15677,\n",
       "  2011,\n",
       "  1996,\n",
       "  3899,\n",
       "  1005,\n",
       "  1055,\n",
       "  4454,\n",
       "  1010,\n",
       "  2021,\n",
       "  2009,\n",
       "  5064,\n",
       "  7208,\n",
       "  1999,\n",
       "  2007,\n",
       "  1996,\n",
       "  9916,\n",
       "  1998,\n",
       "  1996,\n",
       "  4034,\n",
       "  2003,\n",
       "  2574,\n",
       "  2006,\n",
       "  2010,\n",
       "  5725,\n",
       "  2205,\n",
       "  1012,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1026,\n",
       "  7987,\n",
       "  1013,\n",
       "  1028,\n",
       "  1996,\n",
       "  5896,\n",
       "  2005,\n",
       "  2023,\n",
       "  2143,\n",
       "  2001,\n",
       "  2761,\n",
       "  2517,\n",
       "  2011,\n",
       "  2703,\n",
       "  5292,\n",
       "  13871,\n",
       "  2483,\n",
       "  1010,\n",
       "  2040,\n",
       "  2101,\n",
       "  4487,\n",
       "  6499,\n",
       "  7962,\n",
       "  2098,\n",
       "  2009,\n",
       "  1012,\n",
       "  1045,\n",
       "  2123,\n",
       "  1005,\n",
       "  1056,\n",
       "  2113,\n",
       "  2339,\n",
       "  1996,\n",
       "  3015,\n",
       "  2182,\n",
       "  2003,\n",
       "  7880,\n",
       "  2379,\n",
       "  2004,\n",
       "  9951,\n",
       "  2004,\n",
       "  2010,\n",
       "  2432,\n",
       "  2718,\n",
       "  5823,\n",
       "  999,\n",
       "  4312,\n",
       "  1010,\n",
       "  1996,\n",
       "  2364,\n",
       "  3114,\n",
       "  2023,\n",
       "  2143,\n",
       "  2573,\n",
       "  2003,\n",
       "  17319,\n",
       "  1996,\n",
       "  3899,\n",
       "  1010,\n",
       "  2040,\n",
       "  4998,\n",
       "  2013,\n",
       "  2108,\n",
       "  2738,\n",
       "  10140,\n",
       "  1010,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1) # ÏòàÏ∏°Í∞íÍ≥º Ïã§Ï†ú Î†àÏù¥Î∏î Îç∞Ïù¥ÌÑ∞Î•º ÌäúÌîåÎ°ú ÏûÖÎ†•\n",
    "\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "id2label = {0:'negative', 1:'positive'}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model,\n",
    "    num_labels=len(label2id),\n",
    "    label2id=label2id,\n",
    "    id2label=id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inseop/Desktop/fastapi-backend-bertmodel/.venv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Î™®Îç∏ ÌïôÏäµÏùÑ ÏúÑÌïú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ÏôÄ ÏÑ§Ï†ï Ï†ïÏùò\n",
    "args = TrainingArguments(\n",
    "    output_dir='train_dir',               # ÌïôÏäµ Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï† ÎîîÎ†âÌÑ∞Î¶¨\n",
    "    overwrite_output_dir=True,            # Ï∂úÎ†• ÎîîÎ†âÌÑ∞Î¶¨Ïóê Ïù¥ÎØ∏ ÏûàÎäî ÌååÏùºÏùÑ ÎçÆÏñ¥Ïì∏ÏßÄ Ïó¨Î∂Ä\n",
    "    num_train_epochs=3,                   # ÌïôÏäµÌï† ÏóêÌè¨ÌÅ¨(epoch) Ïàò\n",
    "    learning_rate=2e-5,                   # ÌïôÏäµÎ•† (learning rate)\n",
    "    per_device_train_batch_size=32,       # Í∞Å ÎîîÎ∞îÏù¥Ïä§(Ïòà: GPU)Îãπ ÌïôÏäµ Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "    per_device_eval_batch_size=32,        # Í∞Å ÎîîÎ∞îÏù¥Ïä§Îãπ ÌèâÍ∞Ä Î∞∞Ïπò ÌÅ¨Í∏∞\n",
    "    evaluation_strategy='epoch'           # ÌèâÍ∞Ä Ï†ÑÎûµ (Ïó¨Í∏∞ÏÑúÎäî Îß§ ÏóêÌè¨ÌÅ¨ÎßàÎã§ ÌèâÍ∞Ä)\n",
    ")\n",
    "\n",
    "# Trainer Í∞ùÏ≤¥Î•º ÏÉùÏÑ±ÌïòÏó¨ ÌïôÏäµ Î∞è ÌèâÍ∞ÄÎ•º Í¥ÄÎ¶¨\n",
    "trainer = Trainer(\n",
    "    model=model,                          # ÌïôÏäµÌï† Î™®Îç∏\n",
    "    args=args,                            # ÌïôÏäµ ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\n",
    "    train_dataset=dataset['train'],       # ÌïôÏäµÏóê ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    eval_dataset=dataset['test'],         # ÌèâÍ∞ÄÏóê ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    compute_metrics=compute_metrics,      # ÌèâÍ∞Ä ÏßÄÌëúÎ•º Í≥ÑÏÇ∞ÌïòÎäî Ìï®Ïàò\n",
    "    tokenizer=tokenizer                   # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä (ÌÖçÏä§Ìä∏Î•º ÌÜ†ÌÅ∞ÏúºÎ°ú Î≥ÄÌôòÌïòÎäî ÎèÑÍµ¨)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 500/3282 [02:33<13:21,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.472, 'grad_norm': 10.08832836151123, 'learning_rate': 1.695307739183425e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñà       | 1000/3282 [04:59<11:12,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3659, 'grad_norm': 7.427433967590332, 'learning_rate': 1.3906154783668494e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1094/3282 [06:04<21:44,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3036186397075653, 'eval_accuracy': 0.8712, 'eval_runtime': 36.317, 'eval_samples_per_second': 413.03, 'eval_steps_per_second': 12.914, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1500/3282 [08:01<08:29,  3.50it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3148, 'grad_norm': 9.545378684997559, 'learning_rate': 1.0859232175502743e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2000/3282 [10:25<06:06,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2927, 'grad_norm': 18.911134719848633, 'learning_rate': 7.81230956733699e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2188/3282 [11:55<04:47,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2777292728424072, 'eval_accuracy': 0.8813333333333333, 'eval_runtime': 35.2961, 'eval_samples_per_second': 424.976, 'eval_steps_per_second': 13.288, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2500/3282 [13:24<03:44,  3.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2765, 'grad_norm': 18.128141403198242, 'learning_rate': 4.765386959171238e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 3000/3282 [15:48<01:19,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2618, 'grad_norm': 12.661847114562988, 'learning_rate': 1.7184643510054846e-06, 'epoch': 2.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3282/3282 [17:45<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27656206488609314, 'eval_accuracy': 0.8856, 'eval_runtime': 35.628, 'eval_samples_per_second': 421.017, 'eval_steps_per_second': 13.164, 'epoch': 3.0}\n",
      "{'train_runtime': 1065.8079, 'train_samples_per_second': 98.517, 'train_steps_per_second': 3.079, 'train_loss': 0.3241748635468143, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3282, training_loss=0.3241748635468143, metrics={'train_runtime': 1065.8079, 'train_samples_per_second': 98.517, 'train_steps_per_second': 3.079, 'total_flos': 882184338000000.0, 'train_loss': 0.3241748635468143, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU vs GPUÏùò Ï∞®Ïù¥\n",
    "\n",
    "# Í∞ÑÎã®Ìïú ÎçßÏÖà Î¨∏Ï†ú 100Í∞ú Î¨∏Ï†úÎ•º ÎàÑÍ∞Ä Îçî Îπ®Î¶¨ ÌíÄÍπåÏöî?\n",
    "# - ÎåÄÌïôÏÉù 1Î™Ö(CPU) vs Ï¥àÎî© 100Î™Ö(GPU)\n",
    "# - NVIDIAÍ∞Ä Ïôú ÎØ∏ÏπúÎìØÏù¥ Ïò¨ÎûêÏ£†? => GPUÎßåÎì§ ÏûñÏïÑÏöî. \n",
    "\n",
    "# GPUÎûë Îî•Îü¨Îãù(Îã®Ïàú ÌñâÎ†¨ Í≥ÑÏÇ∞)ÏùÄ Î≠î ÏÉÅÍ¥ÄÏù¥ÏßÄ?\n",
    "# - Í∞ÑÎã®Ìïú ÎçßÏÖà Î¨∏Ï†ú Ìë∏ÎäîÎç∞ ÎåÄÌïôÏÉù 1Î™Ö (ÏãúÍ∏â 100ÎßåÏõê) -> Ï¥àÎî©100Î™Ö(ÏãúÍ∏â1ÎßåÏõê)\n",
    "\n",
    "# Îã§Íµ¥Ïóî Ïû•ÏÇ¨ÏóÜÎã§. => Ìè¨Ìè¥Ïù¥ Ï°∏Îùº ÎßéÏúºÎ©¥ ÎêòÏöî. Ìè¨Ìè¥ 1Í∞ú Ïù∏Í≤É Î≥¥Îã§ 2Í∞ú => 3Í∞ú => 4Í∞ú // Ïï± 10Í∞ú(Í≥ÑÏÇ∞Í∏∞...) => ÏΩîÌÖå ÏïàÎ¥ÖÎãàÎã§.\n",
    "# ÌöåÏÇ¨ÏóêÏÑú Î©îÏùºÏù¥ Ïò¨ Îïå ÏΩîÌÖå(ÌïÑÌÑ∞) ÏóÜÏù¥ Î∞îÎ°ú Î©¥Ï†ëÏùÑ Î≥¥ÏûêÍ≥† Ìï¥Ïöî.\n",
    "# ÏΩîÌÖå 30Î∂Ñ ÎØ∏ÎßåÏúºÎ°ú Î®∏Î¶¨ ÏãùÌûàÎäî Ïö©ÏúºÎ°ú => ÎÇòÎ®∏ÏßÄ ÏßÑÏßú Í≥µÎ∂ÄÎ•º ÌïòÏÑ∏Ïöî.\n",
    "# DE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('tinybert-sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "classifier = pipeline('text-classification', model='tinybert-sentiment-analysis', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.6487680673599243},\n",
       " {'label': 'positive', 'score': 0.7922708988189697},\n",
       " {'label': 'positive', 'score': 0.9265885353088379},\n",
       " {'label': 'positive', 'score': 0.6664153337478638}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    \"The Chinese WW2 spy thriller ‚ÄúDecoded‚Äù stands out for a number of reasons, mostly in spite of its conventional and hackneyed depiction of a troubled mathematician who deciphers encrypted messages for the mainland army. For starters, ‚ÄúDecoded‚Äù provides a dramatic change of pace for two marquee-worthy names: soft-spoken heart-throb Liu Haoran, who takes an unusual leading man role as the gifted, but painfully shy codebreaker Rong Jinzhen; and director Chen Sicheng, who‚Äôs best known for his goofy mega-blockbuster ‚ÄúDetective Chinatown‚Äù comedies. With ‚ÄúDecoded,‚Äù a plodding adaptation of Mai Jia‚Äôs popular source novel, Chen and Liu abandon cheap-seats humor‚ÄîLiu co-starred in the ‚ÄúDetective Chinatown‚Äù movies, playing a straight man to comedian Wang Baoqiang‚Äîto pursue a more sober, but less convincing type of cornball power fantasy.\",\n",
    "    \"Liu also played a frustrated, but superhumanly gifted wallflower in ‚ÄúDetective Chinatown.‚Äù He was more convincing in those movies, partly because he was part of a winning buddy duo, but also because he wasn‚Äôt trying to capital-A act while wearing hairpieces, whose synthetic hairs thin at an alarming rate as his character ages. As Jinzhen, Liu brings to mind Russell Crowe‚Äôs performance as the schizophrenic mathematician John Nash in ‚ÄúA Beautiful Mind.‚Äù That association gets harder and harder to shake as Jinzhen inevitably loses his grip on reality while trying to solve the Black Cipher, a nigh-impossible encryption key that was specifically designed to stump Jinzhen.\",\n",
    "    \"Liu‚Äôs mostly compelling as a leading man whenever he can suggest a lot about Jinzhen by speaking softly and deferring his gaze, as if Jinzhen expects to be reprimanded or inconvenienced at any time. He‚Äôs still often eclipsed by co-star John Cusack, whose broad and twitchy performance often distracts from his dialogue, as well as a series of campy dream sequences that ostensibly speak for Liu‚Äôs introverted protagonist.\",\n",
    "    \"Liu doesn‚Äôt exactly light up the screen in his limited capacity as a humanoid plot device. His character either reacts to or follows after whatever promising new development might help Jinzhen to solve the latest problem that‚Äôs vexing him. The filmmakers do what they can to compensate for their unlikely hero‚Äôs prevailing lack of charm and agency, but not even the combined forces of Lloyd Dobler and the Fab Four can bring a spike of joy to this DOA period drama.\"\n",
    "]\n",
    "\n",
    "classifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9783684015274048}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[\n",
    "    \"\"\"When are you guys going to fix all the issues?? Firstly, none of the reaction emojis are showing up. It's just a grey circle. When scrolling, it doesn't move freely. There's like a delay!! Very frustrating!!! Also, nearly every post is either from a \"suggested page\" or a \"sponsered page\". I hardly ever see anything from the pages that I actually follow or my friends pages. No wonder so many people are leaving FB üôÑüôÑ\"\"\"\n",
    "]\n",
    "\n",
    "classifier(data)\n",
    "\n",
    "# ÌîÑÎ°úÍ∑∏Îû®: Íµ¨Í∏Ä ÌîåÎ†àÏù¥ Ïä§ÌÜ†Ïñ¥ ÎßÅÌÅ¨Î•º ÎÑ£ÏúºÎ©¥ => Î¶¨Î∑∞ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤¥ ÌÅ¨Î°§ÎßÅ => Î∂ÄÏ†ïÏùò Í∞ïÎèÑÍ∞Ä 0.8 Ïù¥ÏÉÅÏù∏ ÏùòÍ≤¨Îßå ÌïÑÌÑ∞ Í±∏Ïñ¥ÏÑú Í≥†Í∞ùÏÇ¨ÏóêÍ≤å Í≥µÏú†\n",
    "# Î≥Ñ1Í∞úÏóê ÎÑ§Í±∞Ìã∞Î∏å 0.8Ïù¥ÏÉÅ => slack ÏúºÎ°ú ÏïåÎ¶ºÎ≥¥ÎÇ¥ => ÎåÄÏùë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î™®Îç∏ÏùÑ s3Ïóê ÏóÖÎ°úÎìú\n",
    "# (1) AWS Î°úÍ∑∏Ïù∏ Ìïú Îã§Ïùå => Î≤ÑÌÇ∑ ÏÉùÏÑ±\n",
    "# (2) boto3Î•º ÌôúÏö©Ìï¥ÏÑú ÏΩîÎìú Î≤†Ïù¥Ïä§ s3 ÏÉùÏÑ± Î∞è ÌååÏùº ÏóÖÎ°úÎìú\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS\n",
    "- IAM Í≥ÑÏ†ï / EC2 - pemÌÇ§ ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3') # s3 ÎßåÎì§ Ïàò ÏûàÎäî Í∂åÌïú / Í≥ÑÏ†ï(aws credentials)\n",
    "\n",
    "# s3.list_buckets()\n",
    "# s3.create_bucket()\n",
    "\n",
    "# Ïñ¥Îñ§ ÏõêÎ¶¨Î°ú Ïã§ÌñâÏù¥ ÎêòÎäî Í±∏ÍπåÏöî?\n",
    "\n",
    "# aws credentials\n",
    "# aws configure list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 bucket ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3 = boto3.client('s3') # s3 ÏΩòÏÜîÏóê Ï†ëÏÜç\n",
    "bucket_name = 'inseop-mlmodels' # bucket name\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    response = s3.list_buckets()\n",
    "\n",
    "    bucket_list = []\n",
    "    for buck in response['Buckets']:\n",
    "        bucket_list.append(buck['Name'])\n",
    "\n",
    "    if bucket_name not in bucket_list:\n",
    "        try:\n",
    "            s3.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={'LocationConstraint':'ap-northeast-2'}\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            print('Ïò§Î•ò Î∞úÏÉù :', e)\n",
    "\n",
    "            if e.response['Error']['Code'] == 'BucketAlreadyExists':    \n",
    "                print('Îã§Î•∏ Î≤ÑÌÇ∑ Ïù¥Î¶ÑÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî.')\n",
    "\n",
    "            elif e.reponse['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "                print('Ïù¥ÎØ∏ ÎßåÎì§Ïñ¥ Ï†∏ÏûàÎäî Î≤ÑÌÇ∑ÏûÖÎãàÎã§.')\n",
    "                \n",
    "            else:\n",
    "                print('Î≤ÑÌÇ∑ ÎßåÎì§Í∏∞Î•º Ïû¨ÏãúÎèÑ Ï§ëÏûÖÎãàÎã§.')\n",
    "                time.sleep(3)\n",
    "                create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bucket('inseop-mlmodelss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE => Îç∞Ïù¥ÌÑ∞Í∞Ä Ïú†Ïã§ÎêòÏóàÏùÑ Îïå Ïñ¥ÎñªÍ≤å Î≥µÍµ¨Ìï† Í≤ÉÏù∏Í∞Ä\n",
    "# AB180 => Îß®ÎÇ† Î≥µÍµ¨. ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨ÏôÄÏÑú => Îã§Ïãú Ïû¨Í∞ÄÍ≥µ => Îã§Ïãú DB insert\n",
    "# ÌïòÎ£®, ÌïúÏãúÍ∞Ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3Ïóê ÌååÏùº(Ìè¥Îçî) ÏóÖÎ°úÎìú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'inseop-mlmodels'\n",
    "\n",
    "# s3.upload_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for root, dir, files in os.walk('tinybert-sentiment-analysis'):\n",
    "    # print(root, dir, files)\n",
    "\n",
    "    for file_name in files:\n",
    "        # print(file_name)\n",
    "\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        print(file_path)\n",
    "    \n",
    "        # s3.upload_file(file_path, bucket_name, file_name)\n",
    "        s3.upload_file(file_path, bucket_name, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def s3_upload_file_folder_name(model_folder, folder_name):\n",
    "    for root, dir, files in os.walk(model_folder):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            s3_key = os.path.join(folder_name, file_name)\n",
    "            s3.upload_file(file_path, bucket_name, s3_key)\n",
    "            # EC2ÏóêÎã§Í∞Ä docker => EC2 - AWS CLI(credentialsÏùÄ ÌïÑÏöîX) + Docker pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_upload_file_folder_name('tinybert-sentiment-analysis', 'tinybert-test-folder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
